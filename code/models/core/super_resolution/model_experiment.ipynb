{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af719db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from ESTFormer import ESTFormer\n",
    "\n",
    "sys.path.append('../../')\n",
    "from utils.epoch_data_module import SuperResEpochDataModule\n",
    "\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d330b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file s:\\PolySecLabProjects\\eeg-image-decoding\\code\\utils\\..\\..\\data\\all-joined-1\\eeg\\preprocessed\\ground-truth\\subj01_session1_eeg.fif...\n",
      "    Range : 1121 ... 1777926 =      2.189 ...  3472.512 secs\n",
      "Ready.\n",
      "3839 events found on stim channel Status\n",
      "Event IDs: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n",
      " 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n",
      " 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n",
      " 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n",
      " 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n",
      " 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n",
      " 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n",
      " 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n",
      " 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n",
      " 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612\n",
      " 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\n",
      " 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648\n",
      " 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666\n",
      " 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684\n",
      " 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702\n",
      " 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720\n",
      " 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738\n",
      " 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756\n",
      " 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774\n",
      " 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792\n",
      " 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810\n",
      " 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828\n",
      " 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846\n",
      " 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864\n",
      " 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882\n",
      " 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900\n",
      " 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918\n",
      " 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936\n",
      " 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954\n",
      " 955 956 957 958 959 960]\n",
      "Not setting metadata\n",
      "3839 matching events found\n",
      "Setting baseline interval to [-0.05078125, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Loading data for 3839 events and 339 original time points ...\n",
      "1 bad epochs dropped\n",
      "Opening raw data file s:\\PolySecLabProjects\\eeg-image-decoding\\code\\utils\\..\\..\\data\\all-joined-1\\eeg\\preprocessed\\ground-truth\\subj01_session1_eeg.fif...\n",
      "    Range : 1121 ... 1777926 =      2.189 ...  3472.512 secs\n",
      "Ready.\n",
      "3839 events found on stim channel Status\n",
      "Event IDs: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n",
      " 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n",
      " 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n",
      " 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n",
      " 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n",
      " 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n",
      " 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n",
      " 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n",
      " 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n",
      " 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612\n",
      " 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\n",
      " 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648\n",
      " 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666\n",
      " 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684\n",
      " 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702\n",
      " 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720\n",
      " 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738\n",
      " 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756\n",
      " 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774\n",
      " 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792\n",
      " 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810\n",
      " 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828\n",
      " 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846\n",
      " 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864\n",
      " 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882\n",
      " 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900\n",
      " 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918\n",
      " 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936\n",
      " 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954\n",
      " 955 956 957 958 959 960]\n",
      "Not setting metadata\n",
      "3839 matching events found\n",
      "Setting baseline interval to [-0.05078125, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Loading data for 3839 events and 339 original time points ...\n",
      "1 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "# data module configuration\n",
    "subject_session_id = 'subj01_session1'\n",
    "resample_freq = 256\n",
    "\n",
    "hi_res_channel_names = [\n",
    "    'TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', \n",
    "    'Iz', 'Oz', 'POz', 'Pz', 'CPz', \n",
    "    'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2'\n",
    "]\n",
    "\n",
    "lo_res_channel_count = 14\n",
    "hi_res_channel_count = len(hi_res_channel_names)\n",
    "\n",
    "if lo_res_channel_count == 4:\n",
    "    lo_res_channel_names = [\n",
    "        'PO7',\n",
    "        'POz', 'Oz',\n",
    "        'PO8',\n",
    "    ]\n",
    "\n",
    "if lo_res_channel_count == 8:\n",
    "    lo_res_channel_names = [\n",
    "        'P7', 'PO7', 'O1',\n",
    "        'POz', 'Oz',\n",
    "        'P8', 'PO8', 'O2'\n",
    "    ]\n",
    "\n",
    "if lo_res_channel_count == 14:\n",
    "    lo_res_channel_names = [\n",
    "        'CP3', 'P3', 'P7', 'PO7', 'O1',\n",
    "        'CPz', 'Pz', 'POz', 'Oz',\n",
    "        'CP4', 'P4', 'P8', 'PO8', 'O2'\n",
    "    ]\n",
    "\n",
    "\n",
    "data_module = SuperResEpochDataModule(lo_res_channel_names=lo_res_channel_names, hi_res_channel_names=hi_res_channel_names, subject_session_id='subj01_session1', resample_freq=resample_freq, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2632e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "time_steps = data_module.lo_res_data_reader[0][0].shape[1]\n",
    "model = ESTFormer(lo_res_channel_names=lo_res_channel_names, hi_res_channel_names=hi_res_channel_names, time_steps=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfdad6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\lightning_fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "s:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='check_points/',\n",
    "    mode='min',\n",
    "    filename=f'{subject_session_id}_PO_650ms_{lo_res_channel_count}_{hi_res_channel_count}_{{epoch:02d}}_{{val_loss:.2f}}',\n",
    "    auto_insert_metric_name=False,\n",
    ")\n",
    "\n",
    "lr_rate_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\", # Auto select the best hardware accelerator available\n",
    "    devices=\"auto\", # Auto select available devices for the accelerator (For eg. mutiple GPUs)\n",
    "    strategy=\"auto\", # Auto select the distributed training strategy.\n",
    "    max_epochs=20, # Maximum number of epoch to train for.\n",
    "    deterministic=True, # For deteministic and reproducible training.\n",
    "    enable_model_summary=True,\n",
    "    callbacks=[model_checkpoint, lr_rate_monitor],  # Declaring callbacks to use.\n",
    "    precision=\"16\", # Using Mixed Precision training.\n",
    "    logger=True, # Auto generate TensorBoard logs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28717337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type                           | Params | Mode \n",
      "-------------------------------------------------------------------------------\n",
      "0  | sigmas             | SigmaParameters                | 2      | train\n",
      "1  | sim                | SIM                            | 324 K  | train\n",
      "2  | trm                | TRM                            | 14.7 K | train\n",
      "3  | norm               | LayerNorm                      | 9.8 K  | train\n",
      "4  | mean_train_loss    | MeanMetric                     | 0      | train\n",
      "5  | mean_val_loss      | MeanMetric                     | 0      | train\n",
      "6  | mean_test_loss     | MeanMetric                     | 0      | train\n",
      "7  | mean_train_snr     | SignalNoiseRatio               | 0      | train\n",
      "8  | mean_val_snr       | SignalNoiseRatio               | 0      | train\n",
      "9  | mean_test_snr      | SignalNoiseRatio               | 0      | train\n",
      "10 | mean_train_mae     | MeanAbsoluteError              | 0      | train\n",
      "11 | mean_val_mae       | MeanAbsoluteError              | 0      | train\n",
      "12 | mean_test_mae      | MeanAbsoluteError              | 0      | train\n",
      "13 | mean_train_mse     | MeanSquaredError               | 0      | train\n",
      "14 | mean_val_mse       | MeanSquaredError               | 0      | train\n",
      "15 | mean_test_mse      | MeanSquaredError               | 0      | train\n",
      "16 | mean_train_nrmse   | NormalizedRootMeanSquaredError | 0      | train\n",
      "17 | mean_val_nrmse     | NormalizedRootMeanSquaredError | 0      | train\n",
      "18 | mean_test_nrmse    | NormalizedRootMeanSquaredError | 0      | train\n",
      "19 | mean_train_pearson | PearsonCorrCoef                | 0      | train\n",
      "20 | mean_val_pearson   | PearsonCorrCoef                | 0      | train\n",
      "21 | mean_test_pearson  | PearsonCorrCoef                | 0      | train\n",
      "   | other params       | n/a                            | 102    | n/a  \n",
      "-------------------------------------------------------------------------------\n",
      "348 K     Trainable params\n",
      "0         Non-trainable params\n",
      "348 K     Total params\n",
      "1.395     Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected both predictions and target to be either 1- or 2-dimensional tensors, but got 3 and 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1080\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:145\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:437\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    431\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m step_args = (\n\u001b[32m    433\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    436\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\code\\lightning\\estformer\\ESTFormer.py:395\u001b[39m, in \u001b[36mESTFormer.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28mself\u001b[39m.mean_val_mse(super_res, hi_res)\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m.mean_val_nrmse(super_res, hi_res)\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_val_pearson\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuper_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhi_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\torchmetrics\\metric.py:313\u001b[39m, in \u001b[36mMetric.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TorchMetricsUserError(\n\u001b[32m    309\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe Metric shouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be synced when performing ``forward``. HINT: Did you forget to call ``unsync`` ?.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.full_state_update \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.full_state_update \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dist_sync_on_step:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28mself\u001b[39m._forward_cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_full_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    315\u001b[39m     \u001b[38;5;28mself\u001b[39m._forward_cache = \u001b[38;5;28mself\u001b[39m._forward_reduce_state_update(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\torchmetrics\\metric.py:328\u001b[39m, in \u001b[36mMetric._forward_full_state_update\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward computation using two calls to `update`.\u001b[39;00m\n\u001b[32m    321\u001b[39m \n\u001b[32m    322\u001b[39m \u001b[33;03mDoing this secures that metrics that need access to the full metric state during `update` works as expected.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m \n\u001b[32m    326\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# global accumulation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m _update_count = \u001b[38;5;28mself\u001b[39m._update_count\n\u001b[32m    331\u001b[39m \u001b[38;5;28mself\u001b[39m._to_sync = \u001b[38;5;28mself\u001b[39m.dist_sync_on_step\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\torchmetrics\\metric.py:549\u001b[39m, in \u001b[36mMetric._wrap_update.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m._enable_grad):\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    551\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected all tensors to be on\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\torchmetrics\\regression\\pearson.py:147\u001b[39m, in \u001b[36mPearsonCorrCoef.update\u001b[39m\u001b[34m(self, preds, target)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    146\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28mself\u001b[39m.mean_x, \u001b[38;5;28mself\u001b[39m.mean_y, \u001b[38;5;28mself\u001b[39m.var_x, \u001b[38;5;28mself\u001b[39m.var_y, \u001b[38;5;28mself\u001b[39m.corr_xy, \u001b[38;5;28mself\u001b[39m.n_total = \u001b[43m_pearson_corrcoef_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcorr_xy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_total\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\torchmetrics\\functional\\regression\\pearson.py:53\u001b[39m, in \u001b[36m_pearson_corrcoef_update\u001b[39m\u001b[34m(preds, target, mean_x, mean_y, var_x, var_y, corr_xy, num_prior, num_outputs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Data checking\u001b[39;00m\n\u001b[32m     52\u001b[39m _check_same_shape(preds, target)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43m_check_data_shape_to_num_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m num_obs = preds.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     55\u001b[39m cond = num_prior.mean() > \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m num_obs == \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decoding\\env\\Lib\\site-packages\\torchmetrics\\functional\\regression\\utils.py:31\u001b[39m, in \u001b[36m_check_data_shape_to_num_outputs\u001b[39m\u001b[34m(preds, target, num_outputs, allow_1d_reshape)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that predictions and target have the correct shape, else raise error.\u001b[39;00m\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preds.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m target.ndim > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     32\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected both predictions and target to be either 1- or 2-dimensional tensors,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m     )\n\u001b[32m     35\u001b[39m cond1 = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_1d_reshape:\n",
      "\u001b[31mValueError\u001b[39m: Expected both predictions and target to be either 1- or 2-dimensional tensors, but got 3 and 3."
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5616b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S:\\\\PolySecLabProjects\\\\eeg-image-decoding\\\\code\\\\lightning\\\\eegnet\\\\check_points\\\\subj01_session1_PO_650ms_29_14_0.43.ckpt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dbdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ESTFormer.load_from_checkpoint(model_checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loader = data_module.test_dataloader()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "for batch in loader:\n",
    "    lo_res, hi_res = batch\n",
    "    \n",
    "    lo_res = lo_res.cuda()\n",
    "    super_res = pretrained.predict(lo_res)\n",
    "\n",
    "    all_preds.append(super_res)\n",
    "    all_targets.append(hi_res)\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)  \n",
    "all_targets = np.concatenate(all_targets, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
