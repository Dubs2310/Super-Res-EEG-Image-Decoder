{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9f7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.hdf5_data_split_generator import HDF5DataSplitGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a281eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_channel_names = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "hr_channel_names = ['Fp1', 'Fz', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 'TP10', 'CP6', 'CP2', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 'F4', 'F8', 'Fp2', 'AF7', 'AF3', 'AFz', 'F1', 'F5', 'FT7', 'FC3', 'C1', 'C5', 'TP7', 'CP3', 'P1', 'P5', 'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'P6', 'P2', 'CPz', 'CP4', 'TP8', 'C6', 'C2', 'FC4', 'FT8', 'F6', 'AF8', 'AF4', 'F2', 'FCz']\n",
    "\n",
    "train_dataset = HDF5DataSplitGenerator(\n",
    "    dataset_type=\"train\",\n",
    "    dataset_split=\"70/25/5\",\n",
    "    eeg_epoch_mode=\"around_evoked_event\",\n",
    "    fixed_length_duration=3,\n",
    "    duration_before_onset=0.05,\n",
    "    duration_after_onset=0.6,\n",
    "    lr_channel_names=lr_channel_names,\n",
    "    hr_channel_names=hr_channel_names,\n",
    "    # subject=1,\n",
    "    # session=1\n",
    ")\n",
    "    \n",
    "val_dataset = HDF5DataSplitGenerator(\n",
    "    dataset_type=\"val\",\n",
    "    dataset_split=\"70/25/5\",\n",
    "    eeg_epoch_mode=\"around_evoked_event\",\n",
    "    fixed_length_duration=3,\n",
    "    duration_before_onset=0.05,\n",
    "    duration_after_onset=0.6,\n",
    "    lr_channel_names=lr_channel_names,\n",
    "    hr_channel_names=hr_channel_names,\n",
    "    # subject=1,\n",
    "    # session=1\n",
    ")\n",
    "\n",
    "test_dataset = HDF5DataSplitGenerator(\n",
    "    dataset_type=\"test\",\n",
    "    dataset_split=\"70/25/5\",\n",
    "    eeg_epoch_mode=\"around_evoked_event\",\n",
    "    fixed_length_duration=3,\n",
    "    duration_before_onset=0.05,\n",
    "    duration_after_onset=0.6,\n",
    "    lr_channel_names=lr_channel_names,\n",
    "    hr_channel_names=hr_channel_names,\n",
    "    # subject=1,\n",
    "    # session=1\n",
    ")\n",
    "    \n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d562cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 350, 70)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4da570e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_before_random_split': np.int64(11912),\n",
       " 'sfreq': np.float64(512.0),\n",
       " 'mode': 'around_evoked_event',\n",
       " 'total_duration': 0.65,\n",
       " 'subject': np.int32(3),\n",
       " 'session': np.int32(2),\n",
       " 'sample_number': np.int32(353700),\n",
       " 'lo_res': array([[ 0.23202977,  0.20920864,  0.14622381, ...,  0.00134021,\n",
       "         -0.15771133, -0.34955075],\n",
       "        [ 0.01221056, -0.02503948, -0.02038559, ...,  0.3969575 ,\n",
       "          0.38693252,  0.23240507],\n",
       "        [ 0.1607427 ,  0.23098284,  0.22597659, ...,  0.94048464,\n",
       "          0.9919666 ,  0.8506374 ],\n",
       "        ...,\n",
       "        [ 0.36972958,  0.28939837,  0.31899136, ..., -0.8681541 ,\n",
       "         -0.9038392 , -0.95805675],\n",
       "        [-0.06118771, -0.19649881, -0.24224031, ..., -0.67309356,\n",
       "         -0.75212777, -0.5791296 ],\n",
       "        [ 0.25438666,  0.22789586,  0.20551479, ..., -0.7824524 ,\n",
       "         -0.7823532 , -1.0273788 ]], dtype=float32),\n",
       " 'hi_res': array([[ 0.22763331,  0.17271604,  0.0899248 , ..., -0.02852346,\n",
       "         -0.15296294, -0.31382313],\n",
       "        [ 0.954421  ,  0.90387696,  0.6918902 , ..., -1.1428118 ,\n",
       "         -1.2798389 , -1.412823  ],\n",
       "        [ 0.1607427 ,  0.23098284,  0.22597659, ...,  0.94048464,\n",
       "          0.9919666 ,  0.8506374 ],\n",
       "        ...,\n",
       "        [ 0.25438666,  0.22789586,  0.20551479, ..., -0.7824524 ,\n",
       "         -0.7823532 , -1.0273788 ],\n",
       "        [ 0.35341576,  0.473393  ,  0.5082222 , ..., -1.5022725 ,\n",
       "         -1.4555787 , -1.534446  ],\n",
       "        [ 0.9844165 ,  1.1658572 ,  1.2223698 , ..., -1.19906   ,\n",
       "         -1.1995867 , -1.1783572 ]], dtype=float32),\n",
       " 'one_hot_encoding': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'category_index': {'accessory': 0,\n",
       "  'animal': 1,\n",
       "  'appliance': 2,\n",
       "  'electronic': 3,\n",
       "  'food': 4,\n",
       "  'furniture': 5,\n",
       "  'indoor': 6,\n",
       "  'kitchen': 7,\n",
       "  'outdoor': 8,\n",
       "  'person': 9,\n",
       "  'sports': 10,\n",
       "  'vehicle': 11}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0068937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_before_random_split': tensor([15645, 37958, 30889, 33205, 38299,   199,  4266, 43883, 16668, 44145,\n",
      "        12024, 22598, 23097, 15023, 11618, 27193, 25504, 36730, 24712, 39117,\n",
      "        20831, 27668, 18128, 11137, 24034, 29408,  6451, 22046, 18615, 26143,\n",
      "        25806,  7154]), 'sfreq': tensor([512., 512., 512., 512., 512., 512., 512., 512., 512., 512., 512., 512.,\n",
      "        512., 512., 512., 512., 512., 512., 512., 512., 512., 512., 512., 512.,\n",
      "        512., 512., 512., 512., 512., 512., 512., 512.], dtype=torch.float64), 'mode': ['around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event', 'around_evoked_event'], 'total_duration': tensor([0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500,\n",
      "        0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500,\n",
      "        0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500, 0.6500,\n",
      "        0.6500, 0.6500, 0.6500, 0.6500, 0.6500], dtype=torch.float64), 'subject': tensor([4, 7, 6, 6, 7, 1, 1, 8, 4, 8, 3, 5, 5, 4, 3, 5, 5, 6, 5, 7, 4, 5, 4, 2,\n",
      "        5, 5, 1, 4, 4, 5, 5, 1], dtype=torch.int32), 'session': tensor([1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1,\n",
      "        1, 2, 2, 2, 2, 2, 1, 2], dtype=torch.int32), 'sample_number': tensor([ 352119,  353674,  533041, 1636253,  510586,  117370,  269021, 1392083,\n",
      "         825907, 1514324,  403344,  154561,  385747,   51770,  214439,  555006,\n",
      "        1524116, 1539286, 1158299,  887842, 1093605,  776298, 1527036, 1754722,\n",
      "         820925, 1611888, 1309529, 1658554,   18954,   54561, 1664927, 1637861],\n",
      "       dtype=torch.int32), 'lo_res': tensor([[[-0.7628, -0.6250, -0.3010,  ..., -1.7940, -1.6640, -1.5328],\n",
      "         [-0.1307, -0.0957, -0.0809,  ...,  0.7930,  0.7143,  0.6652],\n",
      "         [-0.6093, -0.4805, -0.2152,  ..., -2.1553, -2.1904, -2.1816],\n",
      "         ...,\n",
      "         [-0.0125, -0.0593,  0.0328,  ..., -1.8783, -1.6658, -1.3844],\n",
      "         [ 0.2715,  0.0489, -0.0942,  ...,  0.1697,  0.1548,  0.1907],\n",
      "         [-0.3334, -0.3929, -0.2090,  ..., -1.4562, -1.3607, -1.0930]],\n",
      "\n",
      "        [[ 0.8264,  0.4725,  0.2393,  ...,  0.8902,  0.5748,  0.1925],\n",
      "         [ 0.0123, -0.4062, -0.4075,  ..., -0.0135, -0.1050, -0.4700],\n",
      "         [ 0.6026,  0.2966,  0.1828,  ...,  0.8599,  0.6488,  0.2620],\n",
      "         ...,\n",
      "         [ 0.7629,  0.8901,  0.9820,  ...,  1.1076,  1.2842,  0.9697],\n",
      "         [-0.0644,  0.1702,  0.4180,  ..., -0.8563, -0.4103, -0.1085],\n",
      "         [ 0.8109,  0.6142,  0.4193,  ...,  1.9030,  1.8664,  1.4721]],\n",
      "\n",
      "        [[-0.6996, -0.4661, -0.1075,  ..., -0.2824, -0.5063, -0.6601],\n",
      "         [ 0.0029, -0.0774, -0.0038,  ...,  0.5511,  0.3865,  0.1315],\n",
      "         [-0.3829, -0.3177, -0.1394,  ...,  0.0816, -0.1973, -0.4789],\n",
      "         ...,\n",
      "         [-0.1264, -0.3491, -0.4428,  ...,  0.0149, -0.1948, -0.5019],\n",
      "         [ 0.1516, -0.1019, -0.2428,  ..., -0.4092, -0.6098, -0.8262],\n",
      "         [-0.2547, -0.2308, -0.1668,  ...,  0.0213, -0.0586, -0.1335]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0381,  0.0263,  0.1149,  ...,  0.3141,  0.3886,  0.3891],\n",
      "         [-0.3807, -0.3319, -0.0997,  ...,  0.0445,  0.4315,  0.5003],\n",
      "         [-0.1404, -0.0931,  0.0868,  ..., -0.1949,  0.1075,  0.4246],\n",
      "         ...,\n",
      "         [-0.0214,  0.0560,  0.0873,  ...,  0.2145,  0.1925,  0.2222],\n",
      "         [ 0.0313,  0.0875,  0.0674,  ..., -0.3791, -0.6276, -0.5757],\n",
      "         [-0.0700, -0.0403, -0.0232,  ..., -0.6762, -0.9160, -0.9426]],\n",
      "\n",
      "        [[ 0.1382,  0.1222,  0.1487,  ..., -0.5711, -0.5461, -0.4812],\n",
      "         [-0.0593, -0.0494,  0.0552,  ..., -0.4792, -0.4760, -0.4246],\n",
      "         [-0.0458, -0.0140,  0.0942,  ..., -0.5252, -0.5142, -0.4457],\n",
      "         ...,\n",
      "         [ 0.4275,  0.3520,  0.3555,  ..., -0.7694, -1.0080, -1.1534],\n",
      "         [ 0.2138,  0.2331,  0.2721,  ..., -0.5238, -0.6806, -0.7747],\n",
      "         [ 0.2515,  0.2559,  0.2755,  ..., -0.3414, -0.4635, -0.5617]],\n",
      "\n",
      "        [[-0.2260, -0.1376, -0.0573,  ...,  0.7863,  0.7502,  0.7380],\n",
      "         [-0.0357, -0.0250, -0.0333,  ..., -1.7984, -1.7903, -1.8054],\n",
      "         [-0.1194, -0.0624, -0.0364,  ..., -0.1033, -0.1295, -0.1500],\n",
      "         ...,\n",
      "         [-0.0574, -0.0491, -0.0242,  ...,  0.9733,  0.9860,  1.0523],\n",
      "         [-0.3061, -0.2820, -0.2228,  ..., -0.5574, -0.5837, -0.5604],\n",
      "         [-0.1802, -0.1533, -0.1581,  ...,  1.2232,  1.1797,  1.1861]]]), 'hi_res': tensor([[[-1.7781e-01, -1.3179e-01, -7.3812e-02,  ...,  4.8746e-01,\n",
      "           4.5279e-01,  4.4747e-01],\n",
      "         [-4.9701e-01, -4.4028e-01, -2.1189e-01,  ..., -1.4251e+00,\n",
      "          -1.3782e+00, -1.2519e+00],\n",
      "         [-6.0926e-01, -4.8045e-01, -2.1519e-01,  ..., -2.1553e+00,\n",
      "          -2.1904e+00, -2.1816e+00],\n",
      "         ...,\n",
      "         [-3.3340e-01, -3.9286e-01, -2.0904e-01,  ..., -1.4562e+00,\n",
      "          -1.3607e+00, -1.0930e+00],\n",
      "         [-1.0210e-01, -1.6889e-01, -1.6318e-02,  ..., -1.4653e+00,\n",
      "          -1.4362e+00, -1.3523e+00],\n",
      "         [-2.1183e-01, -2.5038e-01, -1.7385e-01,  ..., -1.3042e+00,\n",
      "          -1.4018e+00, -1.4096e+00]],\n",
      "\n",
      "        [[ 2.6360e-01,  7.6772e-02,  8.3936e-02,  ...,  7.2028e-01,\n",
      "           6.8014e-01,  4.6008e-01],\n",
      "         [ 1.0738e+00,  8.7258e-01,  7.6609e-01,  ...,  1.7817e+00,\n",
      "           1.7221e+00,  1.4298e+00],\n",
      "         [ 6.0260e-01,  2.9657e-01,  1.8283e-01,  ...,  8.5986e-01,\n",
      "           6.4878e-01,  2.6198e-01],\n",
      "         ...,\n",
      "         [ 8.1090e-01,  6.1418e-01,  4.1934e-01,  ...,  1.9030e+00,\n",
      "           1.8664e+00,  1.4721e+00],\n",
      "         [ 8.2728e-01,  7.3657e-01,  7.1375e-01,  ...,  8.5030e-01,\n",
      "           7.5635e-01,  4.6327e-01],\n",
      "         [ 1.3971e+00,  1.2543e+00,  1.1108e+00,  ...,  1.9002e+00,\n",
      "           1.8453e+00,  1.6254e+00]],\n",
      "\n",
      "        [[-1.9928e-01, -2.4811e-02,  1.2940e-01,  ...,  2.5620e-01,\n",
      "          -1.5086e-01, -5.7941e-01],\n",
      "         [-6.2622e-01, -5.1161e-01, -2.3025e-01,  ...,  4.4712e-01,\n",
      "           3.1237e-01,  1.6648e-01],\n",
      "         [-3.8293e-01, -3.1768e-01, -1.3939e-01,  ...,  8.1583e-02,\n",
      "          -1.9731e-01, -4.7885e-01],\n",
      "         ...,\n",
      "         [-2.5465e-01, -2.3084e-01, -1.6679e-01,  ...,  2.1298e-02,\n",
      "          -5.8602e-02, -1.3351e-01],\n",
      "         [-4.0943e-01, -4.7228e-01, -3.4077e-01,  ..., -4.2039e-01,\n",
      "          -6.2790e-01, -8.7096e-01],\n",
      "         [-1.6639e-01, -2.4349e-01, -2.1989e-01,  ..., -1.7767e-01,\n",
      "          -3.9375e-01, -5.4958e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7144e-01, -1.4045e-01, -6.5572e-02,  ...,  7.9993e-02,\n",
      "           5.0486e-02,  6.5875e-03],\n",
      "         [-6.1214e-02,  6.3847e-02,  2.1644e-01,  ...,  2.6948e-01,\n",
      "           2.4125e-01,  2.7633e-01],\n",
      "         [-1.4041e-01, -9.3108e-02,  8.6841e-02,  ..., -1.9494e-01,\n",
      "           1.0746e-01,  4.2464e-01],\n",
      "         ...,\n",
      "         [-7.0047e-02, -4.0348e-02, -2.3175e-02,  ..., -6.7618e-01,\n",
      "          -9.1603e-01, -9.4263e-01],\n",
      "         [ 7.6649e-02,  2.2473e-01,  3.5233e-01,  ...,  4.6078e-02,\n",
      "          -8.7466e-02, -1.3601e-01],\n",
      "         [-3.9891e-02,  1.5113e-03,  1.0986e-01,  ...,  1.7998e-01,\n",
      "           8.1520e-02, -3.8777e-02]],\n",
      "\n",
      "        [[-8.5070e-02, -3.7794e-02,  2.6356e-02,  ..., -1.2233e-01,\n",
      "          -1.5196e-01, -1.5999e-01],\n",
      "         [ 3.7097e-01,  3.2252e-01,  2.9849e-01,  ..., -7.0518e-01,\n",
      "          -8.1367e-01, -8.5562e-01],\n",
      "         [-4.5787e-02, -1.3989e-02,  9.4229e-02,  ..., -5.2522e-01,\n",
      "          -5.1422e-01, -4.4567e-01],\n",
      "         ...,\n",
      "         [ 2.5151e-01,  2.5590e-01,  2.7548e-01,  ..., -3.4140e-01,\n",
      "          -4.6345e-01, -5.6167e-01],\n",
      "         [ 5.2244e-01,  4.6818e-01,  4.6808e-01,  ..., -1.0005e+00,\n",
      "          -1.1426e+00, -1.2011e+00],\n",
      "         [ 2.3091e-01,  2.4356e-01,  2.7618e-01,  ..., -1.3859e+00,\n",
      "          -1.5117e+00, -1.5221e+00]],\n",
      "\n",
      "        [[-2.6238e-02, -7.8956e-03, -8.8398e-03,  ..., -1.8724e+00,\n",
      "          -1.8802e+00, -1.8770e+00],\n",
      "         [-1.0107e-01, -7.4623e-02, -3.5185e-02,  ...,  1.4193e+00,\n",
      "           1.4066e+00,  1.3890e+00],\n",
      "         [-1.1939e-01, -6.2446e-02, -3.6392e-02,  ..., -1.0332e-01,\n",
      "          -1.2954e-01, -1.4996e-01],\n",
      "         ...,\n",
      "         [-1.8018e-01, -1.5335e-01, -1.5809e-01,  ...,  1.2232e+00,\n",
      "           1.1797e+00,  1.1861e+00],\n",
      "         [-4.0536e-02, -3.8688e-02, -6.7384e-03,  ...,  1.1774e+00,\n",
      "           1.1663e+00,  1.1604e+00],\n",
      "         [ 5.7840e-02,  5.2861e-02,  4.5779e-02,  ...,  1.3846e+00,\n",
      "           1.3917e+00,  1.3743e+00]]]), 'one_hot_encoding': tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
      "        [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 'category_index': {'accessory': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'animal': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'appliance': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2]), 'electronic': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3]), 'food': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4]), 'furniture': tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5]), 'indoor': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6]), 'kitchen': tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7]), 'outdoor': tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8]), 'person': tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9]), 'sports': tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]), 'vehicle': tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11])}}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af0260b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 171, in collate\n    {\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 172, in <dictcomp>\n    key: collate(\n         ^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 285, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 270, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\storage.py\", line 1198, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\storage.py\", line 410, in _new_shared\n    return cls._new_using_filename_cpu(size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn't open shared file mapping: <torch_30184_1417690782_1>, error code: <1455>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mevoked_event_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m960\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1515\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1513\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1514\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1550\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32ms:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 171, in collate\n    {\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 172, in <dictcomp>\n    key: collate(\n         ^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 285, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 270, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\storage.py\", line 1198, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"s:\\PolySecLabProjects\\eeg-image-decode\\env\\Lib\\site-packages\\torch\\storage.py\", line 410, in _new_shared\n    return cls._new_using_filename_cpu(size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Couldn't open shared file mapping: <torch_30184_1417690782_1>, error code: <1455>\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for img_id in batch['evoked_event_id']:\n",
    "        if not (0 <= img_id - 1 < 960):\n",
    "            print(\"Error: \", img_id)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d710d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int32(935)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.get_evoked_event_metadata_for_item(5752)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3040fffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5752, 13206, 10755, ...,  7859,  8297, 10659]),\n",
       " [[8, 1, 747534, 801],\n",
       "  [8, 1, 747936, 795],\n",
       "  [8, 1, 748339, 814],\n",
       "  [8, 1, 748737, 741]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_indices, train_dataset.get_evoked_event_ids_for_item(13206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376b3b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10773)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for index in batch['actual_index']:\n",
    "        print(index)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e43c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196f893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/1006 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress_bar = tqdm(train_loader, desc=f\"Epoch {1}/{2}\", leave=False)\n",
    "\n",
    "# for i, batch in enumerate(progress_bar):\n",
    "#     lo_res = batch['lo_res']\n",
    "#     hi_res = batch['hi_res']\n",
    "\n",
    "#     # if i % 10 == 0:\n",
    "#     progress_bar.set_postfix(loss=f\"{2.79}\", mae=f\"{3.0}\")\n",
    "\n",
    "len(progress_bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1b5531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    # print(batch['lo_res'].shape)\n",
    "    for index in batch['actual_index']:\n",
    "        evoked_ids = train_dataset.get_evoked_event_ids_for_item(index)\n",
    "        print(evoked_ids)\n",
    "    break\n",
    "\n",
    "# train_dataset[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
