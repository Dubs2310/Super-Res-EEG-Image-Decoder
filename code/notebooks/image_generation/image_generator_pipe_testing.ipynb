{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from models.core.diffusion.pipe import Pipe\n",
    "from models.core.diffusion.custom_pipeline import Generator4Embeds\n",
    "from models.core.diffusion.diffusion_prior import DiffusionPriorUNet\n",
    "from utils.data_modules.diffusion_embedding import DiffusionEmbeddingDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aede8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DiffusionEmbeddingDataModule(\n",
    "    eeg_embeddings_file=\"path/to/eeg_embeddings.npy\",\n",
    "    subject=1,\n",
    "    session=1,\n",
    "    batch_size=1024,\n",
    "    num_workers=4,\n",
    "    val_split=0.1,\n",
    "    test='default'\n",
    ")\n",
    "\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23029ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859de453",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "pipe = Pipe(diffusion_prior, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"../models/check_points/diffusion_prior/subj0{1}_session{1}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model_name = 'diffusion_prior' # 'diffusion_prior_vice_pre_imagenet' or 'diffusion_prior_vice_pre'\n",
    "pipe.diffusion_prior.load_state_dict(torch.load(save_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99521792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the image generator\n",
    "print(\"Initializing image generator...\")\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabd26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images from test EEG signals\n",
    "print(\"Generating images from test EEG...\")\n",
    "output_dir = \"generated_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "num_samples_to_generate = min(100, len(test_loader.dataset))\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 5.0\n",
    "\n",
    "for i in range(num_samples_to_generate):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Generating image {i+1}/{num_samples_to_generate}...\")\n",
    "    \n",
    "    # Get EEG embedding for this sample\n",
    "    eeg_embed, _ = test_loader.dataset[i].to(device)\n",
    "    \n",
    "    # Generate image embedding using diffusion prior\n",
    "    generated_img_embed = pipe.generate(\n",
    "        c_embeds=eeg_embed,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "    \n",
    "    for j in range(10):\n",
    "        # Generate actual image\n",
    "        image = generator.generate(generated_img_embed.to(dtype=torch.float16))\n",
    "        \n",
    "        # Save image\n",
    "        image_path = os.path.join(output_dir, f\"generated_image_{i:03d}.png\")\n",
    "        image.save(image_path)\n",
    "        \n",
    "        # Display first 5 images\n",
    "        if i < 5:\n",
    "            print(f\"Generated image {i+1}:\")\n",
    "            display(image)\n",
    "\n",
    "print(f\"All generated images saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with ground truth: Generate image from actual image embeddings\n",
    "print(\"Generating reference images from ground truth image embeddings...\")\n",
    "reference_dir = \"reference_images\"\n",
    "os.makedirs(reference_dir, exist_ok=True)\n",
    "\n",
    "for i in range(min(5, len(test_loader.dataset))):\n",
    "    # Use ground truth image embedding\n",
    "    _, gt_img_embed = test_loader[i:i+1].to(device)\n",
    "    \n",
    "    # Generate image directly from ground truth embedding\n",
    "    reference_image = generator.generate(gt_img_embed.to(dtype=torch.float16))\n",
    "    \n",
    "    # Save reference image\n",
    "    ref_path = os.path.join(reference_dir, f\"reference_image_{i:03d}.png\")\n",
    "    reference_image.save(ref_path)\n",
    "    \n",
    "    print(f\"Reference image {i+1} (ground truth):\")\n",
    "    display(reference_image)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
