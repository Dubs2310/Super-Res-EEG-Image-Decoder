{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from IPython.display import display\n",
    "from torch.utils.data import DataLoader\n",
    "from models.core.diffusion.pipe import Pipe\n",
    "from models.trainers.contrastive import ContrastiveTrainerModel\n",
    "from models.core.diffusion.custom_pipeline import Generator4Embeds\n",
    "from utils.data_modules.contrastive import EEGContrastiveDataModule\n",
    "from models.core.diffusion.diffusion_prior import DiffusionPriorUNet\n",
    "from utils.datasets.diffusion_embedding import DiffusionEmbeddingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aede8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = EEGContrastiveDataModule(\n",
    "    input_channels=['Fp1', 'AF7', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7', 'TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz', 'Pz', 'CPz', 'Fpz', 'Fp2', 'AF8', 'AF4', 'AFz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2'],\n",
    "    sfreq=250,\n",
    "    montage='standard_1020',\n",
    "    window_before_event_ms=50,\n",
    "    window_after_event_ms=600,\n",
    "    subject=1, \n",
    "    session=1, \n",
    "    batch_size=1024, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = dm.get_sample_info()\n",
    "epochs = 150\n",
    "subject = 1\n",
    "session = 1\n",
    "num_channels = sample_data['input']['num_channels']\n",
    "timesteps = sample_data['input']['num_timesteps']\n",
    "num_fine_labels = sample_data['output']['fine_labels_shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ad211",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"S:\\\\PolySecLabProjects\\\\eeg-image-decoding\\\\code\\\\models\\\\check_points\\\\contrastive_encoder\\\\subj1_session1_epoch=199.ckpt\"\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad370d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = ContrastiveTrainerModel(num_channels, timesteps, num_fine_labels)\n",
    "\n",
    "if 'state_dict' in checkpoint:\n",
    "    lightning_model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    lightning_model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_model = lightning_model.to(device)\n",
    "contrastive_model.eval()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "\n",
    "# %%\n",
    "# Extract EEG features using your trained contrastive model\n",
    "print(\"Extracting EEG features from training data...\")\n",
    "contrastive_model.eval()\n",
    "train_features_list = []\n",
    "train_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (eeg_data, output) in enumerate(train_loader):\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Processing batch {batch_idx}/{len(train_loader)}\")\n",
    "        \n",
    "        img, img_features, text_features, super_labels, fine_labels = output\n",
    "        eeg_data = eeg_data.to(device)\n",
    "        \n",
    "        # Extract EEG features using your contrastive encoder\n",
    "        eeg_features = contrastive_model.encoder(eeg_data)  # This gives you the aligned embeddings\n",
    "        \n",
    "        train_features_list.append(eeg_features.cpu())\n",
    "        train_labels_list.append(fine_labels.cpu())\n",
    "\n",
    "# Concatenate all features\n",
    "train_eeg_features = torch.cat(train_features_list, dim=0)\n",
    "train_labels = torch.cat(train_labels_list, dim=0)\n",
    "\n",
    "print(f\"Training EEG features shape: {train_eeg_features.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "\n",
    "# %%\n",
    "# Extract EEG features from test data\n",
    "print(\"Extracting EEG features from test data...\")\n",
    "test_features_list = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (eeg_data, output) in enumerate(test_loader):\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Processing batch {batch_idx}/{len(test_loader)}\")\n",
    "        \n",
    "        img, img_features, text_features, super_labels, fine_labels = output\n",
    "        eeg_data = eeg_data.to(device)\n",
    "        \n",
    "        # Extract EEG features using your contrastive encoder\n",
    "        eeg_features = contrastive_model.encoder(eeg_data)\n",
    "        \n",
    "        test_features_list.append(eeg_features.cpu())\n",
    "        test_labels_list.append(fine_labels.cpu())\n",
    "\n",
    "# Concatenate all features\n",
    "test_eeg_features = torch.cat(test_features_list, dim=0)\n",
    "test_labels = torch.cat(test_labels_list, dim=0)\n",
    "\n",
    "print(f\"Test EEG features shape: {test_eeg_features.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# %%\n",
    "# Load pre-computed image embeddings (ViT-H-14 features)\n",
    "print(\"Loading image embeddings...\")\n",
    "img_embeddings_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "img_embeddings_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "\n",
    "print(f\"Original train image embeddings shape: {img_embeddings_train.shape}\")\n",
    "print(f\"Original test image embeddings shape: {img_embeddings_test.shape}\")\n",
    "\n",
    "# Reshape if needed (based on your original code structure)\n",
    "if len(img_embeddings_train.shape) == 4:  # (num_images, num_repetitions, num_views, embed_dim)\n",
    "    img_embeddings_train_reshaped = img_embeddings_train.view(-1, img_embeddings_train.shape[-1])\n",
    "else:\n",
    "    img_embeddings_train_reshaped = img_embeddings_train\n",
    "\n",
    "print(f\"Reshaped train image embeddings shape: {img_embeddings_train_reshaped.shape}\")\n",
    "\n",
    "# %%\n",
    "# Create dataset for diffusion training\n",
    "print(\"Creating diffusion training dataset...\")\n",
    "diffusion_dataset = DiffusionEmbeddingDataset(\n",
    "    c_embeddings=train_eeg_features, \n",
    "    h_embeddings=img_embeddings_train_reshaped\n",
    ")\n",
    "\n",
    "diffusion_dataloader = DataLoader(\n",
    "    diffusion_dataset, \n",
    "    batch_size=1024, \n",
    "    shuffle=True, \n",
    "    num_workers=64\n",
    ")\n",
    "\n",
    "print(f\"Diffusion dataset size: {len(diffusion_dataset)}\")\n",
    "\n",
    "# %%\n",
    "# Initialize diffusion prior\n",
    "# The cond_dim should match the output dimension of your contrastive encoder\n",
    "encoder_output_dim = train_eeg_features.shape[1]\n",
    "print(f\"Encoder output dimension: {encoder_output_dim}\")\n",
    "\n",
    "diffusion_prior = DiffusionPriorUNet(\n",
    "    cond_dim=encoder_output_dim,  # This should match your encoder output\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Diffusion prior parameters: {sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# %%\n",
    "# Train the diffusion prior\n",
    "print(\"Training diffusion prior...\")\n",
    "pipe = Pipe(diffusion_prior, device=device)\n",
    "pipe.train(diffusion_dataloader, num_epochs=150, learning_rate=1e-3)\n",
    "\n",
    "# %%\n",
    "# Save the trained diffusion prior\n",
    "save_path = \"./checkpoints/diffusion_prior_contrastive.pt\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "torch.save(pipe.diffusion_prior.state_dict(), save_path)\n",
    "print(f\"Diffusion prior saved to {save_path}\")\n",
    "\n",
    "# %%\n",
    "# Initialize the image generator\n",
    "print(\"Initializing image generator...\")\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)\n",
    "\n",
    "# %%\n",
    "# Generate images from test EEG signals\n",
    "print(\"Generating images from test EEG...\")\n",
    "output_dir = \"generated_images_contrastive\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "num_samples_to_generate = min(100, len(test_eeg_features))\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 5.0\n",
    "\n",
    "for i in range(num_samples_to_generate):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Generating image {i+1}/{num_samples_to_generate}...\")\n",
    "    \n",
    "    # Get EEG embedding for this sample\n",
    "    eeg_embed = test_eeg_features[i:i+1].to(device)\n",
    "    \n",
    "    # Generate image embedding using diffusion prior\n",
    "    generated_img_embed = pipe.generate(\n",
    "        c_embeds=eeg_embed,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "    \n",
    "    # Generate actual image\n",
    "    image = generator.generate(generated_img_embed.to(dtype=torch.float16))\n",
    "    \n",
    "    # Save image\n",
    "    image_path = os.path.join(output_dir, f\"generated_image_{i:03d}.png\")\n",
    "    image.save(image_path)\n",
    "    \n",
    "    # Display first 5 images\n",
    "    if i < 5:\n",
    "        print(f\"Generated image {i+1}:\")\n",
    "        display(image)\n",
    "\n",
    "print(f\"All generated images saved to {output_dir}\")\n",
    "\n",
    "# %%\n",
    "# Compare with ground truth: Generate image from actual image embeddings\n",
    "print(\"Generating reference images from ground truth image embeddings...\")\n",
    "reference_dir = \"reference_images\"\n",
    "os.makedirs(reference_dir, exist_ok=True)\n",
    "\n",
    "for i in range(min(5, len(img_embeddings_test))):\n",
    "    # Use ground truth image embedding\n",
    "    gt_img_embed = img_embeddings_test[i:i+1].to(device)\n",
    "    \n",
    "    # Generate image directly from ground truth embedding\n",
    "    reference_image = generator.generate(gt_img_embed.to(dtype=torch.float16))\n",
    "    \n",
    "    # Save reference image\n",
    "    ref_path = os.path.join(reference_dir, f\"reference_image_{i:03d}.png\")\n",
    "    reference_image.save(ref_path)\n",
    "    \n",
    "    print(f\"Reference image {i+1} (ground truth):\")\n",
    "    display(reference_image)\n",
    "\n",
    "# %%\n",
    "# Evaluate reconstruction quality (optional)\n",
    "print(\"Evaluating reconstruction quality...\")\n",
    "\n",
    "# Calculate similarity between generated and ground truth embeddings\n",
    "similarities = []\n",
    "mse_losses = []\n",
    "\n",
    "for i in range(min(50, len(test_eeg_features))):\n",
    "    # Generate embedding from EEG\n",
    "    eeg_embed = test_eeg_features[i:i+1].to(device)\n",
    "    generated_embed = pipe.generate(\n",
    "        c_embeds=eeg_embed,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=5.0\n",
    "    )\n",
    "    \n",
    "    # Get ground truth embedding\n",
    "    gt_embed = img_embeddings_test[i:i+1].to(device)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(\n",
    "        generated_embed, gt_embed, dim=1\n",
    "    ).item()\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = torch.nn.functional.mse_loss(\n",
    "        generated_embed, gt_embed\n",
    "    ).item()\n",
    "    \n",
    "    similarities.append(cos_sim)\n",
    "    mse_losses.append(mse)\n",
    "\n",
    "avg_similarity = sum(similarities) / len(similarities)\n",
    "avg_mse = sum(mse_losses) / len(mse_losses)\n",
    "\n",
    "print(f\"Average cosine similarity: {avg_similarity:.4f}\")\n",
    "print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Generate images with different guidance scales (experiment)\n",
    "print(\"Experimenting with different guidance scales...\")\n",
    "experiment_dir = \"guidance_scale_experiment\"\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "sample_idx = 0  # Use first test sample\n",
    "eeg_embed = test_eeg_features[sample_idx:sample_idx+1].to(device)\n",
    "\n",
    "guidance_scales = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "\n",
    "for guidance_scale in guidance_scales:\n",
    "    print(f\"Generating with guidance scale {guidance_scale}...\")\n",
    "    \n",
    "    # Generate image embedding\n",
    "    generated_embed = pipe.generate(\n",
    "        c_embeds=eeg_embed,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "    \n",
    "    # Generate image\n",
    "    image = generator.generate(generated_embed.to(dtype=torch.float16))\n",
    "    \n",
    "    # Save image\n",
    "    image_path = os.path.join(experiment_dir, f\"guidance_{guidance_scale}_sample_{sample_idx}.png\")\n",
    "    image.save(image_path)\n",
    "    \n",
    "    print(f\"Guidance scale {guidance_scale}:\")\n",
    "    display(image)\n",
    "\n",
    "print(\"Guidance scale experiment completed!\")\n",
    "\n",
    "# %%\n",
    "# Optional: Load pre-trained diffusion prior for inference only\n",
    "# Uncomment if you want to load a pre-trained model instead of training\n",
    "\n",
    "# print(\"Loading pre-trained diffusion prior...\")\n",
    "# diffusion_prior_pretrained = DiffusionPriorUNet(cond_dim=encoder_output_dim, dropout=0.1)\n",
    "# diffusion_prior_pretrained.load_state_dict(torch.load(\"./checkpoints/diffusion_prior_contrastive.pt\"))\n",
    "# pipe_pretrained = Pipe(diffusion_prior_pretrained, device=device)\n",
    "\n",
    "# # Use pipe_pretrained for inference\n",
    "# # ...\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
